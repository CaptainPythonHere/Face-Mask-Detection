{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f217de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from keras_preprocessing import image\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88a97c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 files belonging to 2 classes.\n",
      "Using 1101 files for training.\n",
      "Found 1376 files belonging to 2 classes.\n",
      "Using 275 files for validation.\n"
     ]
    }
   ],
   "source": [
    "folders = tf.io.gfile.glob(\"data\")\n",
    "img = tf.io.gfile.glob(\"data\\*\\*.jpg\")\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  folders[0],\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  )\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  folders[0],\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  )\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Conv2D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dropout(0.7),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf84412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "35/35 [==============================] - 53s 1s/step - loss: 0.9928 - accuracy: 0.5402 - val_loss: 0.6874 - val_accuracy: 0.8909\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 31s 881ms/step - loss: 0.5550 - accuracy: 0.8603 - val_loss: 0.2669 - val_accuracy: 0.9527\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 31s 885ms/step - loss: 0.2638 - accuracy: 0.9588 - val_loss: 0.2522 - val_accuracy: 0.9382\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 32s 891ms/step - loss: 0.2506 - accuracy: 0.9561 - val_loss: 0.2206 - val_accuracy: 0.9673\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 31s 886ms/step - loss: 0.1819 - accuracy: 0.9741 - val_loss: 0.2619 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179ed3004c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5,\n",
    "  callbacks=callbacks_list \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedb62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_1 (Rescaling)      (None, 180, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 178, 178, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 87, 87, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 41, 41, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 121,250\n",
      "Trainable params: 121,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7d9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.built = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d9bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c56164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without_mask\n"
     ]
    }
   ],
   "source": [
    "images = image.load_img(\"dataset/User.22.jpg\", target_size=(img_height, img_width))    \n",
    "x = image.img_to_array(images)\n",
    "#x = tf.image.rgb_to_grayscale(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "#x = x/255.0\n",
    "arr = model.predict(x)\n",
    "print(class_names[np.argmax(arr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39fe8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "global new_array \n",
    "cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    #img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "        #flags=cv2.CV_HAAR_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        saved_img = frame[y:y+h, x:x+w]\n",
    "        count += 1\n",
    "        cv2.imwrite(\"dataset3/User.\" +  \n",
    "                str(count) + \".jpg\", saved_img)\n",
    "        images = image.load_img(\"dataset3/User.\" +  str(count) + \".jpg\",target_size=(180, 180))   \n",
    "        #cv2.imwrite(\"dataset3/User.\" +  \n",
    "            #        str(count) + \".jpg\", images)\n",
    "        #images = cv2.resize(saved_img,dim=(180,180))\n",
    "        face_image = image.img_to_array(images)\n",
    "        face_image = np.expand_dims(face_image, axis=0)\n",
    "        arr = model.predict(face_image)\n",
    "        new_array = face_image\n",
    "        label = np.argmax(arr)\n",
    "        \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255, 0, 0))\n",
    "        cv2.rectangle(frame,(x,y-40),(x+w,y),(255, 0, 0))\n",
    "        cv2.putText(frame,class_names[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bae87a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0., 255.,   0.],\n",
       "         [  0., 255.,   0.],\n",
       "         [  0., 255.,   0.],\n",
       "         ...,\n",
       "         [  0., 255.,   0.],\n",
       "         [  0., 255.,   0.],\n",
       "         [  0., 255.,   0.]],\n",
       "\n",
       "        [[  0., 255.,   0.],\n",
       "         [  4., 247.,   3.],\n",
       "         [ 20., 218.,  15.],\n",
       "         ...,\n",
       "         [ 27., 233.,  32.],\n",
       "         [ 28., 233.,  33.],\n",
       "         [  2., 253.,   2.]],\n",
       "\n",
       "        [[  0., 255.,   0.],\n",
       "         [ 20., 217.,  15.],\n",
       "         [ 95.,  76.,  70.],\n",
       "         ...,\n",
       "         [132., 149., 155.],\n",
       "         [135., 150., 158.],\n",
       "         [  9., 248.,  11.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0., 255.,   0.],\n",
       "         [  8., 208.,   7.],\n",
       "         [ 36.,  31.,  34.],\n",
       "         ...,\n",
       "         [128., 136., 141.],\n",
       "         [130., 139., 143.],\n",
       "         [  9., 247.,  10.]],\n",
       "\n",
       "        [[  0., 255.,   0.],\n",
       "         [  7., 208.,   7.],\n",
       "         [ 35.,  31.,  33.],\n",
       "         ...,\n",
       "         [141., 150., 153.],\n",
       "         [132., 142., 145.],\n",
       "         [  9., 247.,  10.]],\n",
       "\n",
       "        [[  0., 255.,   0.],\n",
       "         [  1., 252.,   0.],\n",
       "         [  2., 239.,   2.],\n",
       "         ...,\n",
       "         [ 10., 248.,  11.],\n",
       "         [  9., 247.,  10.],\n",
       "         [  1., 254.,   1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2efdc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ed522f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without_mask\n"
     ]
    }
   ],
   "source": [
    "#images = im.open(\"dataset3/User.107.jpg\")\n",
    "face_image = image.load_img(\"dataset3/User.108.jpg\")  \n",
    "face_image = np.expand_dims(face_image, axis=0)\n",
    "#print(face_image)\n",
    "arr = model.predict(face_image)\n",
    "#new_array = face_image\n",
    "label = class_names[np.argmax(arr)]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acbce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f1960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
